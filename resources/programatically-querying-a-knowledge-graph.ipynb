{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c400f3",
   "metadata": {},
   "source": [
    "# Programatically Querying A Knowledge Graph\n",
    " - Jupyter template file to programmatically query a Knowledge Graph using SPARQL triplestore API endpoint\n",
    " - enables to interact with RDF-based knowledge graphs directly from Python, using programmatic interfaces instead of manual web-based query tools\n",
    "    - Programmatic querying supports automation, repeatability, and scalability in workflows where semantic data needs to be extracted, transformed, or analyzed at regular intervals\n",
    "    - Helps in building software or AI applications which utilize KGs as a knowledge source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd611f-3960-4a7b-95c9-9f58d1087880",
   "metadata": {},
   "source": [
    "## Prerequisite: Fuseki Setup (IMPORTANT!)\n",
    "- Before running this notebook, make sure that [Apache Jena Fuseki](https://jena.apache.org/documentation/fuseki2/) is properly setup with its endpoint information or have it installed locally on your system.\n",
    "- This tool is the server we’ll use to store and query RDF data using SPARQL.\n",
    "### What You Need to Do:\n",
    "#### External Server (Usually will have admin access disabled - so its read-only)\n",
    " - Have your triplestore endpoint url. Example endpoint will look like this `https://stko-kwg.geog.ucsb.edu/sparql`\n",
    "#### Local Server\n",
    " - Install Fuseki from [Apache Jena Fuseki](https://jena.apache.org/download/index.cgi) site.\n",
    " - Set the `JENA_HOME` environment variable to the folder where you installed Fuseki.\n",
    "    - Example on Windows:\n",
    "        ```bat\n",
    "        set JENA_HOME=C:\\Programs\\apache-jena-fuseki\n",
    "        ```\n",
    "    - Example on Linux environments:\n",
    "        ```bash\n",
    "        export JENA_HOME=/Users/yourname/apache-jena-fuseki\n",
    "        ```\n",
    " - Start the server using the command below (from this notebook or a terminal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f556890-967b-49f1-9e10-b238c74d18bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This command attempts to start the Fuseki server from the JENA_HOME path\n",
    "# This command tries to launch the Apache Jena Fuseki server from your local installation.\n",
    "# Make sure the environment variable JENA_HOME points to your Jena installation directory.\n",
    "# If you see an error like 'command not found', it means JENA_HOME might not be set correctly or Fuseki isn't installed.\n",
    "!$JENA_HOME\\fuseki start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cababf6-f0a4-4fbe-9360-c26a61a5306c",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    " - Before we can interact with RDF data or send HTTP requests, we need to make sure a few Python libraries are installed.\n",
    "\n",
    "\n",
    "### What These Do:\n",
    " - SPARQLWrapper: Helps Python talk to SPARQL endpoints (like Fuseki)\n",
    " - requests: Lets Python send web requests (like fetching data from URLs)\n",
    " - pandas: For working with tables and dataframes\n",
    " \n",
    " If you see “Requirement already satisfied,” that’s good news — it means you already have the package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a5d6d-c4a7-4a9b-9019-dbbcecf92265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required Python libraries for working with RDF data and HTTP requests.\n",
    "!pip3 install SPARQLWrapper requests pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e3ad7-ac8c-4d8b-b85f-af46c5fb6734",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    " - Once the packages are installed, we import them into Python so we can use them\n",
    "\n",
    "### Here’s what each one does:\n",
    "- os: Used to access environment variables like `JENA_HOME`\n",
    "- requests: For sending HTTP GET/POST requests\n",
    "- SPARQLWrapper: Core library for querying SPARQL endpoints\n",
    "- pandas: For manipulating tabular data (great for SPARQL results!)\n",
    "- pprint: Makes output prettier and easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9be908-b037-4443-960e-473787e0d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Python libraries for working with RDF data and HTTP requests.\n",
    "import os\n",
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431b024-b724-4093-b8d3-9f1dcb4b3174",
   "metadata": {},
   "source": [
    "### Function to Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d659562-9790-427d-8a01-b32a86ddd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new dataset in Fuseki (It requires the admin URL and dataset name).\n",
    "# The function also allows specifying the type of persistence (e.g., TDB2 or in-memory).\n",
    "def create_fuseki_dataset(admin_url, dataset_name, persistent_type = \"tdb2\"): # Use \"mem\" for in-memory\n",
    "\n",
    "    payload = {\n",
    "        \"dbName\": dataset_name,\n",
    "        \"dbType\": persistent_type\n",
    "    }\n",
    "    \n",
    "    # Sending a SPARQL query to the Fuseki server and handling the response.\n",
    "    response = requests.post(admin_url, data=payload) # sends a POST request to the Fuseki server to create the dataset.\n",
    "\n",
    "    # Checking the response status code and printing appropriate messages.\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Dataset '{dataset_name}' created successfully.\")\n",
    "    elif \"already exists\" in response.text:\n",
    "        print(f\"Dataset '{dataset_name}' already exists.\")\n",
    "    else:\n",
    "        print(\"Error creating dataset:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8cae6d-a8b3-4146-a59b-cae428aef23e",
   "metadata": {},
   "source": [
    "### Function to Upload TTL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785df82-f2af-477e-9cca-b1a47a5c5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload a ttl to a dataset in the Fuseki server.\n",
    "# The function takes the base URL of the Fuseki server, the dataset name, and the file path of the ttl file.\n",
    "def upload_ttl_file(fuseki_base, dataset_name, file_path):\n",
    "    # Constructing the URL for uploading data to the Fuseki server.\n",
    "    data_url = f\"{fuseki_base}/{dataset_name}/data\"\n",
    "    headers = {\"Content-Type\": \"text/turtle\"}\n",
    "\n",
    "    # Reading the ttl file and sending it to the Fuseki server.\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        response = requests.post(data_url, headers=headers, data=f) # sends a POST request to the Fuseki server to upload the data.\n",
    "\n",
    "    # Checking the response status code and printing appropriate messages.\n",
    "    if response.status_code in (200, 201):\n",
    "        print(f\"File '{file_path}' uploaded successfully.\")\n",
    "    else:\n",
    "        print(\"Error uploading file:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476e698-3906-4a18-82e0-02f3e30441b4",
   "metadata": {},
   "source": [
    "## If you want to start from scratch, use the below instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10aed1-8771-48f4-89bc-9829285a8669",
   "metadata": {},
   "source": [
    "### Setting Up Apache Jena Fuseki\n",
    " - Before you can run SPARQL queries or interact with your knowledge graph, you need to have Apache Jena Fuseki set up and running.\n",
    "\n",
    "### Set Fuseki Base and Dataset Name\n",
    " - Have your Fuseki endpoint as a default parameter to the `fuseki_base` variable or set it as an environment variable and extract from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab01d49-20e6-4cb0-b89c-a6266e4e1737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuseki base URL: http://arsenal.cs.wright.edu:3030\n",
      "SPARQL endpoint: http://arsenal.cs.wright.edu:3030/mydataset/sparql\n"
     ]
    }
   ],
   "source": [
    "# Get Fuseki base URL from environment variable\n",
    "fuseki_base = os.getenv(\"JENA_HOME\", \"http://arsenal.cs.wright.edu:3030\")\n",
    "# Default dataset name\n",
    "dataset_name = \"mydataset\"  # Change to your preferred new dataset name\n",
    "\n",
    "# Full endpoint and admin URLs\n",
    "endpoint_url = f\"{fuseki_base}/{dataset_name}/sparql\"\n",
    "admin_url = f\"{fuseki_base}/$/datasets\"\n",
    "\n",
    "print(\"Fuseki base URL:\", fuseki_base)\n",
    "print(\"SPARQL endpoint:\", endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da407ea3-5017-4065-a98d-c05da8c5482c",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e6801-07ea-4b88-9de9-f4d89543a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mydataset' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function call to create a new dataset\n",
    "# You can change the dataset name and persistent type as needed.\n",
    "# For example, use \"mem\" for in-memory datasets or \"tdb2\" for persistent datasets.\n",
    "# create_fuseki_dataset(admin_url, dataset_name, persistent_type=\"mem\")\n",
    "create_fuseki_dataset(admin_url, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c8ccd-8fc9-4413-b51c-eea467f6870a",
   "metadata": {},
   "source": [
    "### Upload Your TTL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb6697-5acb-4fed-b7ce-835ab111349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'example_data.ttl' uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function call to upload a TTL file to the dataset\n",
    "# You can replace \"example_data.ttl\" with the path to your actual TTL file.\n",
    "# Make sure the file exists in the specified path.\n",
    "upload_ttl_file(fuseki_base, dataset_name, \"example_data.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a918ad-4788-4d86-8ad8-1b9b157bdc46",
   "metadata": {},
   "source": [
    "## Use Existing Dataset in Triplestore\n",
    "### If you want to use an existing dataset in fuseki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a3446-291b-4f89-b801-ec9da2960aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Fuseki base URL from environment variable\n",
    "fuseki_base = os.getenv(\"JENA_HOME\", \"http://localhost:3030\")\n",
    "\n",
    "\n",
    "dataset_name = \"mydataset\"  # Change to your exisitng dataset name available in Fuseki\n",
    "\n",
    "# Full endpoint and admin URLs\n",
    "endpoint_url = f\"{fuseki_base}/{dataset_name}/sparql\"\n",
    "\n",
    "print(\"Fuseki base URL:\", fuseki_base)\n",
    "print(\"SPARQL endpoint:\", endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd386b-63d1-4791-8b6d-853d26014801",
   "metadata": {},
   "source": [
    "### Define SPARQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b511256",
   "metadata": {},
   "source": [
    "- This cell defines a SPARQL query to retrieve distinct topics and their names from the dataset.\n",
    "- The query uses the edu-ont ontology to find resources of type 'edu-ont:Topic' and their associated names.\n",
    "- The result is limited to 10 entries for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866277fe-00f4-433d-b645-4c14be09b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SPARQL query to retrieve distinct topics and their names from the dataset.\n",
    "query = \"\"\"\n",
    "PREFIX edu-ont: <https://edugate.cs.wright.edu/lod/ontology/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT DISTINCT ?topic ?topicName\n",
    "WHERE {\n",
    "  ?topic rdf:type edu-ont:Topic ;\n",
    "         edu-ont:asString ?topicName .\n",
    "}\n",
    "LIMIT 10 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c519b-f536-4f2d-81cc-ee5b890f07da",
   "metadata": {},
   "source": [
    "### Run Query with SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3505a5-3841-485a-aefc-217c5a6bcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPARQLWrapper with the SPARQL endpoint URL\n",
    "sparql = SPARQLWrapper(endpoint_url) # This sets up the connection to the Fuseki server for executing SPARQL queries.\n",
    "\n",
    "# Set the SPARQL query to be executed\n",
    "sparql.setQuery(query) # The query is defined in the previous cell and retrieves distinct topics and their names.\n",
    "\n",
    "# Set the return format of the query results to JSON\n",
    "sparql.setReturnFormat(JSON) # JSON format is easier to parse and work with in Python.\n",
    "\n",
    "\n",
    "# Execute the query and convert the results to a Python dictionary\n",
    "results = sparql.query().convert() # This sends the query to the Fuseki server and retrieves the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9387b-8f07-49df-9170-ef3728c0804b",
   "metadata": {},
   "source": [
    "### Show Results as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ad742-1890-4086-9d99-b50ba2772d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>topicName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Data Science Careers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Case Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Math Fundamentals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://edugate.cs.wright.edu/lod/resource/Top...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic  \\\n",
       "0  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "1  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "2  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "3  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "4  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "5  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "6  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "7  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "8  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "9  https://edugate.cs.wright.edu/lod/resource/Top...   \n",
       "\n",
       "                           topicName  \n",
       "0                             Coding  \n",
       "1                             Python  \n",
       "2                             Pandas  \n",
       "3               Data Science Careers  \n",
       "4                        Probability  \n",
       "5                       Case Studies  \n",
       "6                   Machine Learning  \n",
       "7  Natural Language Processing (NLP)  \n",
       "8                  Math Fundamentals  \n",
       "9                         Statistics  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the \"bindings\" section from the SPARQL query results.\n",
    "bindings = results[\"results\"][\"bindings\"] # The \"bindings\" contain the actual data returned by the query.\n",
    "\n",
    "# Transform the bindings into a list of dictionaries.\n",
    "# Each dictionary represents a row of data, where the keys are variable names and the values are their corresponding values.\n",
    "data = [\n",
    "    {var: binding[var][\"value\"] for var in binding} # Extract the \"value\" field for each variable in the binding.\n",
    "    for binding in bindings # Iterate over all bindings (rows of results).\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame.\n",
    "# This makes it easier to work with the data in a tabular format.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame.\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
